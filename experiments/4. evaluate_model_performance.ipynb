{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a1fdd37c-424d-40e2-a6a9-d9ba32702d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../src')\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.io import decode_image\n",
    "\n",
    "from qdrant_client.models import Distance\n",
    "\n",
    "from src.image_encoder import PreTrainedImageEncoder\n",
    "from src.image_process import load_image_and_metadata\n",
    "from src.evaluation import mean_average_precision, extract_retrieved_point_data\n",
    "from src.qdrant_vector_db import QdrantVectorDB\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8848fa53-ff26-477e-a22e-45e6fc6efac2",
   "metadata": {},
   "source": [
    "# 1. Create Ground Truth\n",
    "\n",
    "The ground truth is created according to image pair_id. Images with the same pair_id indicates same clothes (may include different colors and logos). Therefore, it's natural to regard retrieved images with the same pair_id as relevant images, labeling as positive retrieval.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "- query_image_metadata = {'pair_id': 1}\n",
    "- retrieved_image_metadata = [{'pair_id': 1}, {'pair_id': 2}]\n",
    "```\n",
    "Then the first retrieved image is relevant (positive) while the second is not (negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e699e6bf-bec8-4027-bfbe-dd3ed3ae41fd",
   "metadata": {},
   "source": [
    "# 2. Define metrics\n",
    "\n",
    "There are several commonly used metrics in the field of Information Retrieval. Such as:\n",
    "\n",
    "1. Mean Reciprocal rank (MRR)\n",
    "2. Mean average precision (mAP)\n",
    "3. Normalized discounted cumulative gain (nDCG)\n",
    "\n",
    "mAP is selected since the relevance score is binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d03e0395-34a5-4831-af8c-b3421c4e5839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5833)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A toy case of mPA\n",
    "t = torch.tensor(\n",
    "    [[0,1,0,1],\n",
    "     [0,0,0,1],\n",
    "     [1,1,1,0]]\n",
    ")\n",
    "mean_average_precision(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316b74ce-0bd9-4267-85e8-1de0d8d5d674",
   "metadata": {},
   "source": [
    "# 3. Create Evaluation Pipeline\n",
    "\n",
    "## 3.1 Load Image Information and Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "939f6dc5-d8fc-4ffa-a3c2-5c2594f7d9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totally 5000 of images.\n",
      "df index: RangeIndex(start=0, stop=5000, step=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>filename</th>\n",
       "      <th>pair_id</th>\n",
       "      <th>img_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7505</td>\n",
       "      <td>007506.json</td>\n",
       "      <td>1638</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2451</td>\n",
       "      <td>002452.json</td>\n",
       "      <td>517</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6376</td>\n",
       "      <td>006377.json</td>\n",
       "      <td>1377</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     filename  pair_id img_source\n",
       "0   7505  007506.json     1638       user\n",
       "1   2451  002452.json      517       user\n",
       "2   6376  006377.json     1377       user"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import df image info\n",
    "df_image_info = pd.read_csv('../data/validation/file_pair_id.csv')\n",
    "# only use user image as queries\n",
    "df_image_info = df_image_info[df_image_info['img_source']=='user'].sample(5000, random_state=100)\n",
    "print(f\"Totally {df_image_info.shape[0]} of images.\")\n",
    "\n",
    "# reset index\n",
    "df_image_info = df_image_info.reset_index()\n",
    "print(f\"df index: {df_image_info.index}\")\n",
    "df_image_info.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0df9fb2-5958-4438-82b7-8aab527deaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Encoder Loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load image encoder:\n",
    "\n",
    "# cache weights to a project folder\n",
    "os.environ['TORCH_HOME'] = '../cache'\n",
    "# select device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available else torch.device(\"cpu\")\n",
    "\n",
    "# Init model with pre-trained weights\n",
    "pre_trained_weights = ResNet50_Weights.IMAGENET1K_V2\n",
    "model = resnet50(weights=pre_trained_weights)\n",
    "\n",
    "# Create image encoder from pre-trained model\n",
    "image_encoder = PreTrainedImageEncoder(model, device=device)\n",
    "\n",
    "# Set encoder to eval mode\n",
    "image_encoder.eval()\n",
    "\n",
    "# Init transforms\n",
    "image_process = pre_trained_weights.transforms()\n",
    "\n",
    "# Move to gpu if possible\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available else torch.device(\"cpu\")\n",
    "image_encoder.to(device)\n",
    "image_encoder.eval()\n",
    "\n",
    "print(\"Image Encoder Loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f6ee39-3d8f-46ff-925e-1a569e1f1901",
   "metadata": {},
   "source": [
    "## 3.2 Load and Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f10a738d-5275-4932-9eae-e237480681c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '../data/validation/image'\n",
    "metadata_dir = '../data/validation/annos'\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2edc48a3-a03b-411c-aa0f-b3ce30d4ac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embeddings = []\n",
    "metadata_list = []\n",
    "for i in range(0, df_image_info.shape[0], batch_size):\n",
    "    df_tmp = df_image_info[i: i+batch_size]\n",
    "\n",
    "    # Load data\n",
    "    img_tensors = []\n",
    "    for idx, row in df_tmp.iterrows():\n",
    "        image_name = row['filename'].split('.')[0]\n",
    "    \n",
    "        img, md = load_image_and_metadata(image_name, img_dir, metadata_dir)\n",
    "    \n",
    "        # transform img\n",
    "        img = image_process(img)\n",
    "    \n",
    "        img_tensors.append(img)\n",
    "        metadata_list.append(md)\n",
    "\n",
    "    img_tensors = torch.stack(img_tensors).to(device)\n",
    "\n",
    "    # Encode\n",
    "    with torch.no_grad():\n",
    "        model_output = image_encoder(img_tensors)\n",
    "        model_output = model_output.cpu()\n",
    "        img_embeddings.append(model_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4aa8a72e-75d1-429b-8118-a4de5f80d57c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2048)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate image embeddings\n",
    "img_embeddings = torch.concat(img_embeddings)\n",
    "img_embeddings = img_embeddings.numpy()\n",
    "img_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "49719a4d-d32f-4dc2-9027-7531263c77ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings and metadata\n",
    "saving_dir = \"../data/validation/\"\n",
    "with open(\n",
    "    os.path.join(saving_dir, 'eval_embed_rs100.pk'), 'wb'\n",
    ") as f:\n",
    "    pickle.dump(img_embeddings, f)\n",
    "\n",
    "with open(\n",
    "    os.path.join(saving_dir, 'eval_meta_rs100.pk'), 'wb'\n",
    ") as f:\n",
    "    pickle.dump(metadata_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f228651c-22b1-4c3a-bb07-40f7ea3b79e3",
   "metadata": {},
   "source": [
    "## 3.3 Load Vector Db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5d1acbe1-3b31-4600-a450-7075875e4914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/visual-search/experiments/../src/qdrant_vector_db.py:25: UserWarning: Local mode is not recommended for collections with more than 20,000 points. Collection <image_db> contains 21309 points. Consider using Qdrant in Docker or Qdrant Cloud for better performance with large datasets.\n",
      "  client = QdrantClient(path=database_path)\n",
      "INFO:root:Loaded Qdrant collection 'image_db' with dimension 2048 and distance 'Cosine'.\n"
     ]
    }
   ],
   "source": [
    "# Load Vector database\n",
    "# Config\n",
    "db_name = 'image_db'\n",
    "db_path = '../vector_database'\n",
    "db_client = QdrantVectorDB.init_client(db_path)\n",
    "# loaded vector db\n",
    "qdrant_db = QdrantVectorDB.load_db(\n",
    "    database_name=db_name, \n",
    "    database_path=db_path, \n",
    "    client=db_client,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7e6bdf-e9b5-49d6-9e58-c7c8fa23d354",
   "metadata": {},
   "source": [
    "## 3.4 Run Visual Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b073d751-bae4-4bd7-91af-fefc9acce8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return 10 images for each query\n",
    "k = 10\n",
    "\n",
    "\n",
    "retrieving_results = []\n",
    "for i in range(img_embeddings.shape[0]):\n",
    "    \n",
    "    ret_imgs = qdrant_db.query_image(query_vector=img_embeddings[i], k=k)\n",
    "\n",
    "    extra_data = {'query_img_id': i, 'query_img_pair_id': metadata_list[i]['pair_id']}\n",
    "    # Extract data for evaluation\n",
    "    retrieving_results.extend(\n",
    "        [extract_retrieved_point_data(point, extra_data) for point in ret_imgs]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "90d2c62e-2d09-44c9-b08f-a6f228b12f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame(retrieving_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4fd10b87-ae75-4a59-8c03-1a5a889bd1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(saving_dir, 'search_res.pk'), 'wb') as f:\n",
    "    pickle.dump(df_res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0604eba9-4338-4d2d-8e4d-251ee8595ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
